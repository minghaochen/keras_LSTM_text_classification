{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa43c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import Model\n",
    "from keras.layers import Conv1D, Embedding, Input, Bidirectional, CuDNNLSTM, Dense, Concatenate, Masking, LSTM, SpatialDropout1D\n",
    "from keras.layers import BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D, GlobalAvgPool1D, GlobalMaxPooling1D\n",
    "from keras.layers import Subtract, Multiply\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.utils import to_categorical\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876956b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "def fix_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "seed = 2021\n",
    "fix_seed(seed)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89aa4438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39799, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('fact_checking_train.csv', sep='\\t')\n",
    "df_train['claim'] = df_train['author'] +' '+ df_train['claim'] \n",
    "df_test = pd.read_csv('fact_checking_test.csv', sep='\\t')\n",
    "df_test['claim'] = df_test['author'] +' '+ df_test['claim'] \n",
    "evidence = pd.read_csv('evidence.csv',sep='\\t')\n",
    "evidence.columns = ['ID','claim']\n",
    "evidence['author'] = 'NaN'\n",
    "evidence['label'] = -1\n",
    "label_2_v = {'pants-fire':0,'false':1,'barely-true':2,'half-true':3,'mostly-true':4,'true':5}\n",
    "df_train['label'] = df_train['label'].map(label_2_v)\n",
    "\n",
    "df_data = evidence.append(df_train)\n",
    "df_data = df_data.append(df_test)\n",
    "# df_data = df_train.append(df_test)\n",
    "df_data = df_data.reset_index(drop=True)\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a19fe60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>author</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Joe Biden Sanders’ “Medicare for All” plan \"wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>Hillary Clinton McCain \"still thinks it's okay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Facebook posts Says a video shows Iranian rock...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Tom Barrett</td>\n",
       "      <td>Tom Barrett \"No one on my staff has ever been ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>City of Atlanta</td>\n",
       "      <td>City of Atlanta Tyler Perry’s plan to turn a m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID           author                                              claim  \\\n",
       "0   0        Joe Biden  Joe Biden Sanders’ “Medicare for All” plan \"wo...   \n",
       "1   1  Hillary Clinton  Hillary Clinton McCain \"still thinks it's okay...   \n",
       "2   2   Facebook posts  Facebook posts Says a video shows Iranian rock...   \n",
       "3   3      Tom Barrett  Tom Barrett \"No one on my staff has ever been ...   \n",
       "4   4  City of Atlanta  City of Atlanta Tyler Perry’s plan to turn a m...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee52a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "I1 = np.load(\"I1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51e6cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6576, 19510,  1769, 13226,  2348],\n",
       "       [19163, 15663,   747, 17304, 17501],\n",
       "       [17224, 10359, 12239,   368,  7150],\n",
       "       [  973, 12778, 13214,   545, 15582],\n",
       "       [18714,  8887, 16241, 11511, 13224]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd4e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate seqs\n"
     ]
    }
   ],
   "source": [
    "max_words_num = None\n",
    "seq_len = 2000\n",
    "seq_len = 256\n",
    "embedding_dim = 16\n",
    "col = 'claim'\n",
    "\n",
    "print('Generate seqs')\n",
    "os.makedirs('seqs', exist_ok=True)\n",
    "seq_path = 'seqs/seqs_{}_{}.npy'.format(max_words_num, seq_len)\n",
    "word_index_path = 'seqs/word_index_{}_{}.npy'.format(max_words_num, seq_len)\n",
    "if not os.path.exists(seq_path) or not os.path.exists(word_index_path):\n",
    "    tokenizer = text.Tokenizer(num_words=max_words_num, lower=False, filters='')\n",
    "#     tokenizer.fit_on_texts(df_data[col].values.tolist())\n",
    "    tokenizer.fit_on_texts(df_train[col].values.tolist())\n",
    "    seqs = sequence.pad_sequences(tokenizer.texts_to_sequences(df_data[col].values.tolist()), maxlen=seq_len,\n",
    "                                  padding='post', truncating='pre')\n",
    "    word_index = tokenizer.word_index\n",
    "        \n",
    "    np.save(seq_path, seqs)\n",
    "    np.save(word_index_path, word_index)\n",
    "\n",
    "else:\n",
    "    seqs = np.load(seq_path)\n",
    "    word_index = np.load(word_index_path, allow_pickle=True).item()\n",
    "\n",
    "embedding = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "\n",
    "env = seqs[0:20006].copy()\n",
    "seqs = seqs[20006:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e884a320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43021, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1807608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    20006\n",
       " 1.0     4462\n",
       " 3.0     3171\n",
       " 2.0     2980\n",
       " 4.0     2898\n",
       " 5.0     2256\n",
       " 0.0     2233\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d72cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs('sub', exist_ok=True)\n",
    "os.makedirs('prob', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26d6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index = [i for i in range(18000)]\n",
    "# test_index = df_data[df_data['label'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccec4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(emb, seq_len):\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb.shape[0],\n",
    "        output_dim=emb.shape[1],\n",
    "        input_length=seq_len,\n",
    "    )\n",
    "    \n",
    "    seq = Input(shape=(seq_len, ))\n",
    "    seq_emb = emb_layer(seq)\n",
    "    \n",
    "    seq_emb = SpatialDropout1D(rate=0.5)(seq_emb)\n",
    "\n",
    "    lstm = Bidirectional(CuDNNLSTM(50, return_sequences=True))(seq_emb)\n",
    "    \n",
    "    lstm_avg_pool = GlobalAveragePooling1D()(lstm)\n",
    "    lstm_max_pool = GlobalMaxPooling1D()(lstm)\n",
    "    x = Concatenate()([lstm_avg_pool, lstm_max_pool])\n",
    "    \n",
    "    x = Dropout(0.5)(Activation(activation='relu')(BatchNormalization()(Dense(128)(x))))\n",
    "    out = Dense(6, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=seq, outputs=out)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Lookahead(RAdam()), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model_multi_input(emb, seq_len):\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb.shape[0],\n",
    "        output_dim=emb.shape[1],\n",
    "        input_length=seq_len,\n",
    "    )\n",
    "    \n",
    "    seq1 = Input(shape=(seq_len, ))\n",
    "    seq2 = Input(shape=(seq_len, ))\n",
    "    seq_emb1 = emb_layer(seq1)\n",
    "    seq_emb2 = emb_layer(seq2)\n",
    "    \n",
    "    shared_lstm = Bidirectional(CuDNNLSTM(200, return_sequences=True))\n",
    "    \n",
    "    seq_emb1 = SpatialDropout1D(rate=0.5)(seq_emb1)\n",
    "    seq_emb2 = SpatialDropout1D(rate=0.5)(seq_emb2)\n",
    "    \n",
    "    lstm1 = shared_lstm(seq_emb1)\n",
    "    lstm2 = shared_lstm(seq_emb2)    \n",
    "    \n",
    "    lstm_avg_pool1 = GlobalAveragePooling1D()(lstm1)\n",
    "    lstm_max_pool1 = GlobalMaxPooling1D()(lstm1)\n",
    "    lstm_avg_pool2 = GlobalAveragePooling1D()(lstm2)\n",
    "    lstm_max_pool2 = GlobalMaxPooling1D()(lstm2)\n",
    "    lstm_multiply_1 = Multiply()([lstm_avg_pool1,lstm_avg_pool2])\n",
    "    lstm_multiply_2 = Multiply()([lstm_max_pool1,lstm_max_pool2])\n",
    "    lstm_subtract_1 = Subtract()([lstm_avg_pool1,lstm_avg_pool2])\n",
    "    lstm_subtract_2 = Subtract()([lstm_max_pool1,lstm_max_pool2])\n",
    "    \n",
    "    \n",
    "    x = Concatenate()([lstm_avg_pool1, lstm_max_pool1, lstm_avg_pool2, lstm_max_pool2,\n",
    "                      lstm_multiply_1, lstm_multiply_2, lstm_subtract_1, lstm_subtract_2])\n",
    "    \n",
    "    x = Dropout(0.2)(Activation(activation='relu')(BatchNormalization()(Dense(1024)(x))))\n",
    "    out = Dense(6, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[seq1, seq2], outputs=out)\n",
    "\n",
    "#     sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5760863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model_multi_input(embedding, seq_len)\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39904488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.best_val_f1 = 0.\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_true = self.y_val\n",
    "        y_pred = self.model.predict(self.x_val).argmax(axis=1)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        return f1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_f1 = self.evaluate()\n",
    "        if val_f1 > self.best_val_f1:\n",
    "            self.best_val_f1 = val_f1\n",
    "        logs['val_f1'] = val_f1\n",
    "        print(f'val_f1: {val_f1:.5f}, best_val_f1: {self.best_val_f1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b9ee383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\anzhuang\\envs\\tf-1.15\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\anzhuang\\envs\\tf-1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 16200 samples, validate on 1800 samples\n",
      "Epoch 1/30\n",
      "16200/16200 [==============================] - 16s 1ms/step - loss: 1.7041 - accuracy: 0.2562 - val_loss: 1.7657 - val_accuracy: 0.2511\n",
      "val_f1: 0.07173, best_val_f1: 0.07173\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25111, saving model to model/lstm_0.h5\n",
      "Epoch 2/30\n",
      "16200/16200 [==============================] - 14s 842us/step - loss: 1.5911 - accuracy: 0.3240 - val_loss: 1.7358 - val_accuracy: 0.3011\n",
      "val_f1: 0.20477, best_val_f1: 0.20477\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.25111 to 0.30111, saving model to model/lstm_0.h5\n",
      "Epoch 3/30\n",
      "16200/16200 [==============================] - 14s 842us/step - loss: 1.4920 - accuracy: 0.3815 - val_loss: 1.6811 - val_accuracy: 0.3067\n",
      "val_f1: 0.24705, best_val_f1: 0.24705\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.30111 to 0.30667, saving model to model/lstm_0.h5\n",
      "Epoch 4/30\n",
      "16200/16200 [==============================] - 14s 853us/step - loss: 1.3794 - accuracy: 0.4401 - val_loss: 1.6920 - val_accuracy: 0.2667\n",
      "val_f1: 0.24066, best_val_f1: 0.24705\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.30667\n",
      "Epoch 5/30\n",
      "16200/16200 [==============================] - 14s 852us/step - loss: 1.2406 - accuracy: 0.5090 - val_loss: 1.7470 - val_accuracy: 0.2706\n",
      "val_f1: 0.23568, best_val_f1: 0.24705\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.30667\n",
      "Epoch 6/30\n",
      "16200/16200 [==============================] - 14s 852us/step - loss: 1.0679 - accuracy: 0.5923 - val_loss: 1.8762 - val_accuracy: 0.2694\n",
      "val_f1: 0.24021, best_val_f1: 0.24705\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.30667\n",
      "Epoch 7/30\n",
      "16200/16200 [==============================] - 14s 857us/step - loss: 0.8770 - accuracy: 0.6783 - val_loss: 2.0363 - val_accuracy: 0.3000\n",
      "val_f1: 0.27687, best_val_f1: 0.27687\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.30667\n",
      "Epoch 8/30\n",
      "16200/16200 [==============================] - 14s 843us/step - loss: 0.6814 - accuracy: 0.7625 - val_loss: 2.3836 - val_accuracy: 0.2961\n",
      "val_f1: 0.27714, best_val_f1: 0.27714\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.30667\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 9/30\n",
      "16200/16200 [==============================] - 14s 841us/step - loss: 0.5019 - accuracy: 0.8397 - val_loss: 2.7532 - val_accuracy: 0.2956\n",
      "val_f1: 0.27262, best_val_f1: 0.27714\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.30667\n",
      "Epoch 10/30\n",
      "16200/16200 [==============================] - 14s 842us/step - loss: 0.4205 - accuracy: 0.8680 - val_loss: 2.7399 - val_accuracy: 0.2956\n",
      "val_f1: 0.27364, best_val_f1: 0.27714\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.30667\n",
      "Epoch 11/30\n",
      "16200/16200 [==============================] - 14s 848us/step - loss: 0.3519 - accuracy: 0.8928 - val_loss: 2.9274 - val_accuracy: 0.3033\n",
      "val_f1: 0.29602, best_val_f1: 0.29602\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.30667\n",
      "Epoch 12/30\n",
      "16200/16200 [==============================] - 14s 837us/step - loss: 0.2962 - accuracy: 0.9135 - val_loss: 3.1872 - val_accuracy: 0.3017\n",
      "val_f1: 0.28152, best_val_f1: 0.29602\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.30667\n",
      "Epoch 13/30\n",
      "16200/16200 [==============================] - 14s 843us/step - loss: 0.2454 - accuracy: 0.9322 - val_loss: 3.4476 - val_accuracy: 0.2922\n",
      "val_f1: 0.28650, best_val_f1: 0.29602\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.30667\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/30\n",
      "16200/16200 [==============================] - 14s 844us/step - loss: 0.1976 - accuracy: 0.9515 - val_loss: 3.7236 - val_accuracy: 0.2883\n",
      "val_f1: 0.27029, best_val_f1: 0.29602\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.30667\n",
      "Epoch 15/30\n",
      "16200/16200 [==============================] - 14s 838us/step - loss: 0.1766 - accuracy: 0.9572 - val_loss: 3.4986 - val_accuracy: 0.2983\n",
      "val_f1: 0.29304, best_val_f1: 0.29602\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.30667\n",
      "Epoch 16/30\n",
      "16200/16200 [==============================] - 14s 846us/step - loss: 0.1614 - accuracy: 0.9628 - val_loss: 3.7200 - val_accuracy: 0.3028\n",
      "val_f1: 0.29640, best_val_f1: 0.29640\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.30667\n",
      "Epoch 17/30\n",
      "16200/16200 [==============================] - 14s 841us/step - loss: 0.1459 - accuracy: 0.9668 - val_loss: 3.7122 - val_accuracy: 0.2994\n",
      "val_f1: 0.29141, best_val_f1: 0.29640\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.30667\n",
      "Epoch 18/30\n",
      "16200/16200 [==============================] - 14s 845us/step - loss: 0.1324 - accuracy: 0.9707 - val_loss: 3.8961 - val_accuracy: 0.2967\n",
      "val_f1: 0.28783, best_val_f1: 0.29640\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.30667\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 19/30\n",
      "16200/16200 [==============================] - 14s 838us/step - loss: 0.1186 - accuracy: 0.9749 - val_loss: 3.9758 - val_accuracy: 0.2978\n",
      "val_f1: 0.28852, best_val_f1: 0.29640\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.30667\n",
      "Epoch 20/30\n",
      "16200/16200 [==============================] - 14s 838us/step - loss: 0.1106 - accuracy: 0.9778 - val_loss: 4.0178 - val_accuracy: 0.2933\n",
      "val_f1: 0.27942, best_val_f1: 0.29640\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.30667\n",
      "Epoch 21/30\n",
      "16200/16200 [==============================] - 14s 845us/step - loss: 0.1041 - accuracy: 0.9809 - val_loss: 4.0033 - val_accuracy: 0.2928\n",
      "val_f1: 0.28624, best_val_f1: 0.29640\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.30667\n",
      "Epoch 22/30\n",
      "16200/16200 [==============================] - 14s 840us/step - loss: 0.1007 - accuracy: 0.9809 - val_loss: 4.0852 - val_accuracy: 0.2922\n",
      "val_f1: 0.28615, best_val_f1: 0.29640\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.30667\n",
      "Epoch 23/30\n",
      "16200/16200 [==============================] - 14s 837us/step - loss: 0.0953 - accuracy: 0.9830 - val_loss: 4.3472 - val_accuracy: 0.2950\n",
      "val_f1: 0.29334, best_val_f1: 0.29640\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.30667\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "bs = 1024\n",
    "monitor = 'val_accuracy'\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "for fold_id, (train_index, val_index) in enumerate(kfold.split(all_index, df_train['label'])):\n",
    "    train_x = seqs[train_index]\n",
    "    val_x = seqs[val_index]\n",
    "    \n",
    "    train_env = env[I1[train_index][:,0]]\n",
    "    val_env = env[I1[val_index][:,0]]\n",
    "\n",
    "    label = df_train['label'].values\n",
    "    train_y = label[train_index]\n",
    "    val_y = label[val_index]\n",
    "    \n",
    "    model_path = 'model/lstm_{}.h5'.format(fold_id)\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor=monitor, verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "    earlystopping = EarlyStopping(monitor=monitor, patience=20, verbose=1, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=5, mode='max', verbose=1)\n",
    "    \n",
    "#     model = build_model(embedding, seq_len)\n",
    "#     model.fit(train_x, train_y, batch_size=bs, epochs=30,\n",
    "#               validation_data=(val_x, val_y),\n",
    "#               callbacks=[Evaluator(validation_data=(val_x, val_y)), checkpoint, reduce_lr, earlystopping], verbose=1, shuffle=True)\n",
    "    \n",
    "    model = build_model_multi_input(embedding, seq_len)\n",
    "    model.fit([train_x,train_env], train_y, batch_size=bs, epochs=30,\n",
    "              validation_data=([val_x,val_env], val_y),\n",
    "              callbacks=[Evaluator(validation_data=([val_x,val_env], val_y)), checkpoint, reduce_lr, earlystopping], verbose=1, shuffle=True)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04538be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
